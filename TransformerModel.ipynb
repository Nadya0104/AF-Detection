{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c9aa6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "\n",
    "class PPGDataset(Dataset):\n",
    "    def __init__(self, folder_path, signal_type='PPG', context_length=500, sample_rate=50):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        af_path = os.path.join(folder_path, 'AF')\n",
    "        for filename in os.listdir(af_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(af_path, filename))\n",
    "                signal = df[signal_type].values\n",
    "                signal = self.resample_signal(signal, sample_rate) # Resample to 50 Hz\n",
    "                signal = self.tokenize_signal(signal) # Tokenize the signal (0-100 range)\n",
    "                windows = self.create_sliding_windows(signal, context_length) # Create sliding windows\n",
    "                # Add labels (1 for AF)\n",
    "                self.data.extend(windows)\n",
    "                self.labels.extend([1] * len(windows))\n",
    "        non_af_path = os.path.join(folder_path, 'Non_AF')\n",
    "        for filename in os.listdir(non_af_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(non_af_path, filename))\n",
    "                signal = df[signal_type].values\n",
    "                signal = self.resample_signal(signal, sample_rate) # Resample to 50 Hz\n",
    "                signal = self.tokenize_signal(signal)  # Tokenize the signal (0-100 range)\n",
    "                windows = self.create_sliding_windows(signal, context_length) # Create sliding windows\n",
    "                # Add labels (0 for Non-AF)\n",
    "                self.data.extend(windows)\n",
    "                self.labels.extend([0] * len(windows))\n",
    "\n",
    "    def resample_signal(self, signal, sample_rate):\n",
    "        # Resample signal to specified rate (50 Hz for PPG)\n",
    "        original_rate = 125  # original sampling rate\n",
    "        step = original_rate // sample_rate\n",
    "        return signal[::step]\n",
    "\n",
    "    def tokenize_signal(self, signal):\n",
    "        # Scale to 0-100 range and round to integer tokens\n",
    "        signal_min, signal_max = signal.min(), signal.max()\n",
    "        scaled_signal = (signal - signal_min) / (signal_max - signal_min) * 100\n",
    "        return np.round(scaled_signal).astype(int)\n",
    "\n",
    "    def create_sliding_windows(self, signal, context_length, stride=50):\n",
    "        windows = []\n",
    "        for i in range(0, len(signal) - context_length + 1, stride):\n",
    "            windows.append(signal[i:i+context_length])\n",
    "        return windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=101, d_model=4, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.token_embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch_size, d_model]\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Take the last token and classify\n",
    "        x = x[-1, :, :]  # [batch_size, d_model]\n",
    "        x = self.fc(x)  # [batch_size, 1]\n",
    "\n",
    "        # Squeeze the output to match the shape of the target\n",
    "        return x.squeeze(-1)  # [batch_size,] instead of [batch_size, 1]\n",
    "\n",
    "def train_model(dataset_path, epochs=5, batch_size=2, learning_rate=3e-4):\n",
    "    # Create dataset\n",
    "    full_dataset = PPGDataset(dataset_path)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss already combines sigmoid and BCELoss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)  # Outputs are now of shape [batch_size,]\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, batch_y)  # No need for .squeeze() here\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)  # No need for .squeeze() here\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = (outputs > 0.5).float()  # For binary classification, threshold at 0.5\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "                total += batch_y.size(0)\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(test_loader):.4f}')\n",
    "        print(f'Validation Accuracy: {100 * correct/total:.2f}%\\n')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "if __name__ == '__main__':\n",
    "    dataset_path = 'Dataset'\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    trained_model = train_model(dataset_path)\n",
    "\n",
    "    # Optional: Save the model\n",
    "    torch.save(trained_model.state_dict(), 'mimic_transformer_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
