Transformer Model - Final Test Report
==================================================

Model Configuration:
--------------------
dataset_path: Dataset
epochs: 20
batch_size: 32
learning_rate: 0.0001
d_model: 64
num_heads: 8
num_layers: 4
context_length: 500
stride: 50
sample_rate: 50
weight_decay: 1e-05
use_scheduler: True
n_folds: 5
val_ratio: 0.2
test_ratio: 0.2
random_seed: 42
n_train_patients: 21
n_val_patients: 7
n_test_patients: 7
n_train_windows: 25000
n_val_windows: 8323
n_test_windows: 8336

Window-Level Performance (Test Set):
-------------------------------------
Accuracy:  0.6519
Precision: 0.6655
Recall:    0.7856
F1 Score:  0.7206
AUC:       0.6478

Patient-Level Performance (Test Set):
----------------------------------
Accuracy:  0.7143
Precision: 0.6667
Recall:    1.0000
F1 Score:  0.8000
AUC:       0.8333

Summary:
--------
This transformer model uses temporal patterns in PPG signals
for AF detection. Patient-level splitting ensures no data leakage
between training, validation, and test sets. The model was selected
based on 5-fold cross-validation performance on the training data.
